---
phase: 1
plan: 3
wave: 2
depends_on: [1, 2]
files_modified: [app/components/confidence-coach/AudioAnalyzer.js, app/components/confidence-coach/ConfidenceCoachUI.jsx]
autonomous: true

must_haves:
  truths:
    - "Web Audio API tracks voice pitch and volume"
    - "Ending the speech aggregates all data into a holistic score 0-10"
  artifacts:
    - "app/components/confidence-coach/AudioAnalyzer.js"
---

# Plan 1.3: Web Audio API & Holistic Scoring

<objective>
Implement voice metrics tracking (volume/pitch) via the Web Audio API and combine it with previous MediaPipe data to produce the final confidence score when the user ends the speech.
Purpose: Completes the real-time multi-modal analysis suite for Phase 1.
Output: Audio analyzer helper and integration into the main UI for final score reduction.
</objective>

<context>
Load for context:
- app/components/confidence-coach/ConfidenceCoachUI.jsx
- app/components/confidence-coach/MediaPipeAnalyzer.js
</context>

<tasks>

<task type="auto">
  <name>Implement Audio Analyzer</name>
  <files>app/components/confidence-coach/AudioAnalyzer.js</files>
  <action>
    Create a helper wrapping Web Audio API `AudioContext` and `AnalyserNode`.
    Capture the microphone stream and compute average volume and a basic frequency representation (pitch estimation) on an interval while speech is active.
    Export a start/stop function returning aggregated voice quality metrics.
    AVOID: Leaking AudioContexts. Ensure it's closed/suspended when the session ends.
  </action>
  <verify>npm run lint or verify no runtime syntax errors</verify>
  <done>Helper correctly connects to `navigator.mediaDevices.getUserMedia` audio stream and logs volume.</done>
</task>

<task type="auto">
  <name>Aggregate Holistic Score</name>
  <files>app/components/confidence-coach/ConfidenceCoachUI.jsx</files>
  <action>
    On "END This Speech", stop the video media tracks, stop MediaPipe, and stop the AudioAnalyzer.
    Take the accumulated stats (face visibility, pose openness, voice volume stability) and run a simple aggregate algorithm to compute a score between 0.0 and 10.0.
    In the UI, show a loading spinner briefly, then display the final score instead of the video feed.
    AVOID: Overcomplicating the math for V1. Use simple thresholds (e.g., face present > 80% time = good, volume variance moderate = good).
  </action>
  <verify>React component compiles and handles the end speech state transition</verify>
  <done>Session can be formally ended, transitioning user to a result score screen without server calls yet.</done>
</task>

</tasks>

<verification>
After all tasks, verify:
- [ ] AudioContext initiates when session starts.
- [ ] Ending the session successfully calculates and renders a score out of 10.
</verification>

<success_criteria>
- [ ] All tasks verified
- [ ] Must-haves confirmed
</success_criteria>
