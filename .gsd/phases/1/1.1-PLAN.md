---
phase: 1
plan: 1
wave: 1
depends_on: []
files_modified: [app/confidence-coach/page.jsx, app/components/confidence-coach/ConfidenceCoachUI.jsx]
autonomous: true

must_haves:
  truths:
    - "Confidence Coach page exists and renders"
    - "Voice commands 'Start' and 'END This Speech' are recognized"
  artifacts:
    - "app/confidence-coach/page.jsx"
    - "app/components/confidence-coach/ConfidenceCoachUI.jsx"
---

# Plan 1.1: Base UI Layout & Voice Command Activation

<objective>
Setup the core Next.js page route and React component for the Confidence Coach, splitting the view into video and instructions. Implement basic Web Speech API to listen for Start/End keywords.
Purpose: Establish the foundation for multimodal tracking.
Output: Core UI component with camera access and speech recognition active.
</objective>

<context>
Load for context:
- .gsd/SPEC.md
- .gsd/ARCHITECTURE.md
</context>

<tasks>

<task type="auto">
  <name>Create Confidence Coach base page & layout</name>
  <files>app/confidence-coach/page.jsx, app/components/confidence-coach/ConfidenceCoachUI.jsx</files>
  <action>
    Create the `/confidence-coach` route. 
    In `ConfidenceCoachUI.jsx`, build a two-section layout (Left: Video feed, Right: Instructions).
    Setup basic `useRef` and `useEffect` to capture the user's webcam feed and display it in a `<video>` element.
    AVOID: Complex styling initially, focus on getting the camera feed to show reliably. Use standard Tailwind classes.
  </action>
  <verify>curl -s http://localhost:3000/confidence-coach | grep -q "Confidence Coach"</verify>
  <done>Page loads without error and requests camera permission, rendering video footprint.</done>
</task>

<task type="auto">
  <name>Integrate Web Speech API for commands</name>
  <files>app/components/confidence-coach/ConfidenceCoachUI.jsx</files>
  <action>
    Implement `window.SpeechRecognition` (or `webkitSpeechRecognition`).
    Listen for transcripts containing "start" or "end this speech".
    Manage a state `sessionStatus` ('idle', 'analyzing', 'ended').
    Update the UI instructions based on `sessionStatus`.
    AVOID: Relying strictly on exact casing, transform transcript to lower case for keyword matching.
  </action>
  <verify>Load page in browser, accept mic, say 'Start', verify status updates</verify>
  <done>Speaking 'Start' changes UI state to analyzing; 'End this speech' changes it to ended.</done>
</task>

</tasks>

<verification>
After all tasks, verify:
- [ ] Camera stream renders on the left side.
- [ ] Speech recognition correctly identifies the two keywords and updates state.
</verification>

<success_criteria>
- [ ] All tasks verified
- [ ] Must-haves confirmed
</success_criteria>
