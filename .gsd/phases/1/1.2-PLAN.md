---
phase: 1
plan: 2
wave: 1
depends_on: []
files_modified: [app/components/confidence-coach/MediaPipeAnalyzer.js]
autonomous: true

must_haves:
  truths:
    - "MediaPipe Face and Pose detection models load successfully"
    - "Landmarks are extracted from the active video feed"
    - "Standing vs Sitting posture is calculated from pose landmarks"
  artifacts:
    - "app/components/confidence-coach/MediaPipeAnalyzer.js"
---

# Plan 1.2: MediaPipe Face & Pose Integration with Posture Detection

<objective>
To integrate Google's local-browser MediaPipe models for pose and face landmark tracking over the active video stream.
To implement logic differentiating whether the user is sitting or standing.
Purpose: This is the core engine for recognizing facial confidence and dynamically adjusting body language analysis based on posture.
Output: A utility JS module that tracks Face/Pose, determines posture, and updates shared data.
</objective>

<context>
Load for context:
- .gsd/SPEC.md
- app/components/confidence-coach/ConfidenceCoachUI.jsx
</context>

<tasks>

<task type="auto">
  <name>Install MediaPipe Dependencies</name>
  <files>package.json</files>
  <action>
    Install `@mediapipe/tasks-vision`.
    AVOID: Installing the old '@mediapipe/face_mesh' legacy packages. Use the new Tasks Vision API.
  </action>
  <verify>npm ls @mediapipe/tasks-vision</verify>
  <done>Package installed and verifiable in package.json.</done>
</task>

<task type="auto">
  <name>Implement MediaPipe Analyzer with Posture Calculation</name>
  <files>app/components/confidence-coach/MediaPipeAnalyzer.js</files>
  <action>
    Create a helper to load `FaceLandmarker` and `PoseLandmarker` from `@mediapipe/tasks-vision`.
    Export a function `startMediaPipeStream(videoElement, onResults)` that runs a `requestAnimationFrame` loop calling `detectForVideo`.
    In the pose data extraction, calculate joint angles (e.g., hip-to-knee-to-ankle or general vertical alignment of shoulders/hips) to determine a `posture` string ('sitting' or 'standing').
    Pass the detected face shape, pose skeleton data, and the `posture` determination back via `onResults`.
    AVOID: Re-instantiating the models every frame. Load them once and use `detectForVideo`.
  </action>
  <verify>npm run lint or verify no syntax errors building this file</verify>
  <done>Module successfully hooks into a video element, provides continuous detection results, and outputs a 'sitting' or 'standing' status.</done>
</task>

</tasks>

<verification>
After all tasks, verify:
- [ ] @mediapipe/tasks-vision dependency exists.
- [ ] Analyzer successfully initializes without throwing bundle/WASM errors.
- [ ] Analyzer successfully calculates and returns a sitting/standing status.
</verification>

<success_criteria>
- [ ] All tasks verified
- [ ] Must-haves confirmed
</success_criteria>
