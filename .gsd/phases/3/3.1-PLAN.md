# Phase 3, Plan 3.1: Advanced Emotion Detection

## Objective
Enhance the existing `MediaPipeAnalyzer.js` to utilize the `faceBlendshapes` returned by MediaPipe to detect macro-expressions (e.g., smiling, frowning, neutrality) to provide a "Poise" or "Emotion" metric over time during the speech.

## Context
Currently, we simply check `faceBlendshapes.length > 0` to determine `visibleFaceFrames`. We can actively use parameters like `jawOpen`, `mouthSmileLeft`, `mouthSmileRight`, and `browDownLeft`/`Right` to derive an emotion state.

## Implementation Steps
1. **Define `evaluateEmotion(blendshapes)`**: Add this heuristic function in `MediaPipeAnalyzer.js`. It should iterate through the available 52 ARKit blendshapes returned by the `FaceLandmarker`.
    - If `mouthSmileLeft` > 0.5 and `mouthSmileRight` > 0.5 -> "positive"
    - If `browDownLeft` > 0.5 and `browDownRight` > 0.5 -> "tense"
    - Else -> "neutral"
2. **Update `onResults` Payload**: Ensure `emotion: evaluateEmotion(...)` is shipped back downstream to the UI alongside posture.
3. **UI Integration**: In `ConfidenceCoachUI.jsx`:
    - Add `positiveFrames`, `tenseFrames`, and `emotionMeasuredFrames` to `mlStats`.
    - Modify the Final Score calculation in `endSession()` to penalize tension and reward positivity mildly (e.g., up to Â±1.0 points).

## Reversion Warning
Do not break the `standing`/`sitting` logic already established. Make sure to defensively check `blendshapes` existence before evaluating.
