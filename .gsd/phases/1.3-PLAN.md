---
phase: 1
plan: 3
wave: 2
depends_on: [1, 2]
files_modified: [app/components/confidence-coach/AudioAnalyzer.js, app/components/confidence-coach/ConfidenceCoachUI.jsx]
autonomous: true

must_haves:
  truths:
    - "Web Audio API tracks voice pitch and volume"
    - "Ending the speech aggregates all data into a holistic score 0-10"
    - "Elapsed time (timeTaken) is computed upon completion"
  artifacts:
    - "app/components/confidence-coach/AudioAnalyzer.js"
---

# Plan 1.3: Web Audio API & Holistic Scoring Aggregation

<objective>
Implement voice metrics tracking (volume/pitch) via the Web Audio API and combine it with previous MediaPipe data and session timing to produce the final payload and confidence score when the user ends the speech.
Purpose: Completes the real-time multi-modal analysis suite for Phase 1 and prepares the final data structure.
Output: Audio analyzer helper and integration into the main UI for final score calculation and time tracking.
</objective>

<context>
Load for context:
- app/components/confidence-coach/ConfidenceCoachUI.jsx
- app/components/confidence-coach/MediaPipeAnalyzer.js
</context>

<tasks>

<task type="auto">
  <name>Implement Audio Analyzer</name>
  <files>app/components/confidence-coach/AudioAnalyzer.js</files>
  <action>
    Create a helper wrapping Web Audio API `AudioContext` and `AnalyserNode`.
    Capture the microphone stream and compute average volume and a basic frequency representation (pitch estimation) on an interval while speech is active.
    Export a start/stop function returning aggregated voice quality metrics.
    AVOID: Leaking AudioContexts. Ensure it's closed/suspended when the session ends.
  </action>
  <verify>npm run lint or verify no runtime syntax errors</verify>
  <done>Helper correctly connects to `navigator.mediaDevices.getUserMedia` audio stream and logs volume.</done>
</task>

<task type="auto">
  <name>Aggregate Holistic Score and Compute Time</name>
  <files>app/components/confidence-coach/ConfidenceCoachUI.jsx</files>
  <action>
    On "END This Speech", stop the video media tracks, MediaPipe, and AudioAnalyzer.
    Calculate `timeTaken = Math.floor((Date.now() - startTime) / 1000)`.
    Take the accumulated stats (face visibility, pose openness, sitting/standing adjustments, voice volume stability) and run a simple aggregate algorithm to compute a `score` between 0.0 and 10.0.
    In the UI, show a loading spinner briefly, then display the final score instead of the video feed.
    Ensure the component holds the complete data shape in state (moduleId, gameType, sessionId, question, userAnswer, isCorrect: true, score, timeTaken) ready for saving.
    AVOID: Overcomplicating the math for V1. Use simple thresholds based on the sitting/standing context and basic metrics.
  </action>
  <verify>React component compiles and handles the end speech state transition</verify>
  <done>Session can be formally ended, computing timeTaken and score, and preparing the final data structure.</done>
</task>

</tasks>

<verification>
After all tasks, verify:
- [ ] AudioContext initiates when session starts.
- [ ] Ending the session successfully calculates timeTaken and a score out of 10.
- [ ] Component state holds the complete required data payload.
</verification>

<success_criteria>
- [ ] All tasks verified
- [ ] Must-haves confirmed
</success_criteria>
