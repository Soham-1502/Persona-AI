---
phase: 1
plan: 1
wave: 1
depends_on: []
files_modified: [app/confidence-coach/page.jsx, app/components/confidence-coach/ConfidenceCoachUI.jsx]
autonomous: true

must_haves:
  truths:
    - "Confidence Coach page exists and renders"
    - "Voice commands 'Start' and 'END This Speech' are recognized"
    - "Session UUID and start time are recorded at start"
    - "Scenario type is selected before start"
    - "Full transcript is accumulated during session"
  artifacts:
    - "app/confidence-coach/page.jsx"
    - "app/components/confidence-coach/ConfidenceCoachUI.jsx"
---

# Plan 1.1: Base UI, Scenario Selection & Transcript accumulation

<objective>
Setup the core Next.js page route and React component for the Confidence Coach. 
Implement pre-session scenario selection.
Implement Web Speech API to listen for Start/End keywords AND accumulate the full user transcript.
Generate initial session metadata (UUID, start time).
Purpose: Establish the foundation for multimodal tracking and collect required session metadata.
Output: Core UI component with camera access, speech recognition, and metadata tracking.
</objective>

<context>
Load for context:
- .gsd/SPEC.md
- .gsd/ARCHITECTURE.md
</context>

<tasks>

<task type="auto">
  <name>Create Confidence Coach base page & layout</name>
  <files>app/confidence-coach/page.jsx, app/components/confidence-coach/ConfidenceCoachUI.jsx</files>
  <action>
    Create the `/confidence-coach` route. 
    In `ConfidenceCoachUI.jsx`, build a two-section layout (Left: Video feed, Right: Instructions).
    Setup basic `useRef` and `useEffect` to capture the user's webcam feed and display it in a `<video>` element.
    Add a UI element to select a scenario type (e.g., "Job Interview", "Presentation", "Networking") before starting.
    Store this selection in a state variable `question`.
    AVOID: Complex styling initially, focus on getting the camera feed to show reliably. Use standard Tailwind classes.
  </action>
  <verify>curl -s http://localhost:3000/confidence-coach | grep -q "Confidence Coach"</verify>
  <done>Page loads, requests camera permission, shows scenario selector, and renders video footprint.</done>
</task>

<task type="auto">
  <name>Integrate Web Speech API for commands and transcripts</name>
  <files>app/components/confidence-coach/ConfidenceCoachUI.jsx</files>
  <action>
    Implement `window.SpeechRecognition` (or `webkitSpeechRecognition`) with `continuous = true` and `interimResults = true`.
    When "start" is detected: Generate a `sessionId` via `crypto.randomUUID()`, record `startTime = Date.now()`, set `sessionStatus = 'analyzing'`.
    Accumulate all spoken text into a `userAnswer` state variable.
    When "end this speech" is detected: set `sessionStatus = 'ended'`.
    Update the UI instructions based on `sessionStatus`.
    AVOID: Relying strictly on exact casing, transform transcript to lower case for keyword matching. Don't include the command keywords in the final `userAnswer` transcript if possible.
  </action>
  <verify>Load page in browser, select scenario, accept mic, say 'Start', verify status updates, speak sentences, see transcript accumulate.</verify>
  <done>Speaking 'Start' generates UUID/timestamp and starts analysis; transcript accumulates; 'End this speech' ends it.</done>
</task>

</tasks>

<verification>
After all tasks, verify:
- [ ] Camera stream renders on the left side.
- [ ] Scenario selection works.
- [ ] Speech recognition correctly identifies the two keywords, accumulates transcript, generates UUID, and logs start time.
</verification>

<success_criteria>
- [ ] All tasks verified
- [ ] Must-haves confirmed
</success_criteria>
